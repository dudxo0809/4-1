p. 2

Likelihood ??? 

	-> 어떤 값이 관측되었을때 어떤 확률분포에서 왔을지에 대한 확률
	-> 가능도


---------------------------------------------------------------------
p. 3

머신러닝의 과정

	-> 트레이닝데이터를 가지고
	-> 알고리즘을 학습시킴
	-> 테스트데이터를 통해 가정이 잘맞는지 확인



---------------------------------------------------------------------
p. 4

Linear regression의 경우 deterministic한 모델
	-> Hard decision

가우시안등의 확률은 statistic한 모델
	-> Soft decision

ex.) 만약 데이터가 가우시안 PDF를 다른다면 평균과 분산만 알면된다!


---------------------------------------------------------------------
p. 5

만약 가우시안을 따른다고 가정하고

	-> 이 가우시안의 likelihood를 찾으려면?
	-> 일일이 조사해서 경험으로 평균과분산을 구한걸 가우시안함수에 넣으면
	-> 이것이 Likelihood PDF가 된다!



---------------------------------------------------------------------
p. 6

Likelihood가 최대가 되게 하는 Function을 찾아야한다!

	-> 데이터를 여러 함수에 대입해봐야함





---------------------------------------------------------------------
p. 7

Likelihood는 기본적으로 확률값이다.

Maximum Likelihood Estimation?

	-> Likelihood값을 최대화하는 PDF를 찾는 문제를 의미한다!!
	-> 여러 observation에 대한 PDF를 최대로하는 함수를 찾는것

	-> 어떤 PDF에서 관측된 데이터 집합에서 파라미터들을 찾는것!
		-> ex.) 가우시안에서는 mean과 variance를 찾는것

---------------------------------------------------------------------
p. 8

	-> 각 샘플값들이 independent하다고 가정하고
	-> 각 확률값에대한 곱이 최대가 되게한다!


	-> 왜 로그를 취하는가?
	-> 곱을 로그로 바꾸면 덧셈으로 표현되어서 훨씬 수월해짐, 
		-> 확률이 모두 1보다 작으므로 곱하면 너무작아짐


---------------------------------------------------------------------
p. 9

로그형태로 바꿔도 max의 x값은 변하지 않는다


---------------------------------------------------------------------
p. 10

시그마 로그 어쩌구의 값을 최대로 하는 파라미터인 평균과 분산을 찾아야한다.

	-> PDF가 가우시안이라고 가정해보자
	-> 이때 이 가우시안에 맞춰 값이 최대가되게하는 평균과 분산값을 찾아야함 (뮤&시그마)

	-> 그럼 이 두 파라미터를 어떻게 찾냐?
	-> 각 파라미터(뮤와 시그마)에 대해 하나씩 미분해보면 된다!


---------------------------------------------------------------------
p. 11, 12

미분해서 미분값이 0이되는 파라미터를 찾았더니

	-> 뮤의 값(분산)은 : 샘플의 평균값이다!!!
	-> 시그마의 값은 : 샘플의 표준편차 값이다!!!


---------------------------------------------------------------------
p. 13

MLE의 문제점은??

	-> 관측값에 너무 예민하다!
	-> 우연히 관측값이 한쪽에 치우쳐져 나오면 문제가됨

	-> Maximum a Posteriori Estimation 으로 해결 (MAP)

	-> ex.) P(Y|X) = ( P(X|Y) * P(Y) ) / P(X)
	-> Y : 산불이 났는지 나지 않았는지(state), X : 기온
	-> 기온만 가지고 산불이 날지 판단하는것은 (P(X|Y)) 정보가 너무 bias될 수 있다
	-> 따라서 실제 산불이 날 확률인 (P(Y)) 즉 prior를 곱하면 더 정확한 확률값을 구할 수 있다.

	-> ex.)


---------------------------------------------------------------------
p. 14

X가 관측값, Y가 상태
	
	MLE는 현재의 관측값에만 의존하지만
	MAP는 관측값과 prior knowledge 모두 고려한다



---------------------------------------------------------------------
p. 16

어떤 데이터는 여러 가우시안 모델의 혼합으로 표현할 수 있다.







---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
