p. 2

Likelihood ??? 

	-> 어떤 값이 관측되었을때 어떤 확률분포에서 왔을지에 대한 확률
	-> 가능도


---------------------------------------------------------------------
p. 3

머신러닝의 과정

	-> 트레이닝데이터를 가지고
	-> 알고리즘을 학습시킴
	-> 테스트데이터를 통해 가정이 잘맞는지 확인



---------------------------------------------------------------------
p. 4

Linear regression의 경우 deterministic한 모델
	-> Hard decision

가우시안등의 확률은 statistic한 모델
	-> Soft decision

ex.) 만약 데이터가 가우시안 PDF를 다른다면 평균과 분산만 알면된다!


---------------------------------------------------------------------
p. 5

만약 가우시안을 따른다고 가정하고

	-> 이 가우시안의 likelihood를 찾으려면?
	-> 일일이 조사해서 경험으로 평균과분산을 구한걸 가우시안함수에 넣으면
	-> 이것이 Likelihood PDF가 된다!



---------------------------------------------------------------------
p. 6

Likelihood가 최대가 되게 하는 Function을 찾아야한다!

	-> 데이터를 여러 함수에 대입해봐야함





---------------------------------------------------------------------
p. 7

Likelihood는 기본적으로 확률값이다.

Maximum Likelihood Estimation?

	-> Likelihood값을 최대화하는 PDF를 찾는 문제를 의미한다!!
	-> 여러 observation에 대한 PDF를 최대로하는 함수를 찾는것

	-> 어떤 PDF에서 관측된 데이터 집합에서 파라미터들을 찾는것!
		-> ex.) 가우시안에서는 mean과 variance를 찾는것

---------------------------------------------------------------------
p. 8

	-> 각 샘플값들이 independent하다고 가정하고
	-> 각 확률값에대한 곱이 최대가 되게한다!


	-> 왜 로그를 취하는가?
	-> 곱을 로그로 바꾸면 덧셈으로 표현되어서 훨씬 수월해짐, 
		-> 확률이 모두 1보다 작으므로 곱하면 너무작아짐


---------------------------------------------------------------------
p. 9

로그형태로 바꿔도 max의 x값은 변하지 않는다


---------------------------------------------------------------------
p. 10

시그마 로그 어쩌구의 값을 최대로 하는 파라미터인 평균과 분산을 찾아야한다.

	-> PDF가 가우시안이라고 가정해보자
	-> 이때 이 가우시안에 맞춰 값이 최대가되게하는 평균과 분산값을 찾아야함 (뮤&시그마)

	-> 그럼 이 두 파라미터를 어떻게 찾냐?
	-> 각 파라미터(뮤와 시그마)에 대해 하나씩 미분해보면 된다!


---------------------------------------------------------------------
p. 11, 12

미분해서 미분값이 0이되는 파라미터를 찾았더니

	-> 뮤의 값(분산)은 : 샘플의 평균값이다!!!
	-> 시그마의 값은 : 샘플의 표준편차 값이다!!!


---------------------------------------------------------------------
p. 13

MLE의 문제점은??

	-> 관측값에 너무 예민하다!
	-> 우연히 관측값이 한쪽에 치우쳐져 나오면 문제가됨

	-> Maximum a Posteriori Estimation 으로 해결 (MAP)

	-> ex.) P(Y|X) = ( P(X|Y) * P(Y) ) / P(X)
	-> Y : 산불이 났는지 나지 않았는지(state), X : 기온
	-> 기온만 가지고 산불이 날지 판단하는것은 (P(X|Y)) 정보가 너무 bias될 수 있다
	-> 따라서 실제 산불이 날 확률인 (P(Y)) 즉 prior를 곱하면 더 정확한 확률값을 구할 수 있다.

	-> ex.)


---------------------------------------------------------------------
p. 14

X가 관측값, Y가 상태
	
	MLE는 현재의 관측값에만 의존하지만
	MAP는 관측값과 prior knowledge 모두 고려한다



---------------------------------------------------------------------
p. 16 ~

어떤 데이터는 여러 가우시안 모델의 혼합으로 표현할 수 있다.

두  군집의 PDF를 하나의 그래프에 그렸다.

	-> 이때 어떤 데이터 x가 어느 군집에 속하는지 classification하려면
	-> 각 PDF에서의 P(w1|x) 와 P(w2|x) 의 값을 비교해서
	-> 가장 값이 큰 확률을 찾아 classification을 수행한다.

---------------------------------------------------------------------
p. 20

이제 어떤 데이터 x가 w1에 속할 확률이
	w2군집에 속할 확률의 몇배가 되는지에 대한 비율
	-> Likelihood Ratio 를 구해보자
	-> 두 군집 중 한 군집에 속할 확률과 다른군집에 속할확률의 비율

	-> 베이즈 정리를 사용한다!
	-> P(w1|x) : 어떤 데이터 x에 대한 w1집단의 확률
	-> 이를 베이즈 정리를 써서 풀면
	-> P(w1|x) = ( P(x|w1) * P(w1) ) / P(x)  이다!!!
	-> 이제 ( P(x|w1) * P(w1) ) / P(x) 와 ( P(x|w2) * P(w2) ) / P(x)의 확률을 비교한다

	-> ex.) 머리카락의 길이로 남여를 classification하는 문제?
	-> P(x)는 소거되고, 각 PDF에서 P(x|w)를 구할 수 있고
		, Prior인 P(w)를 알고있다면 ratio를 구할 수 있다!!!

	-> 이 식을 풀어 Error가 최소화 되는 boarder값 x를 구할 수 있다.

---------------------------------------------------------------------
p. 22, 23

Error Probability

그래프에서 색칠해진 부분이 모두 에러

	-> R1영역에서는 w1이 확률이 높고
	-> R2영역에서는 w2가 확률이 높음

	-> 에러일 확률 = w1에서 R2일확률 + w2에서 R1일 확률
	
//


---------------------------------------------------------------------
p. 25

Loss Function

	-> 잘못 결정을 내렸을때의 손해!

---------------------------------------------------------------------
p. 26 ~ 

Bayes Risk

	-> 전체 데이터에 대한 cost의 기댓값
	-> C12 : 원래 2인데 1로 잘못판단된 Loss
	-> C21 : 원래 1인데 2로 잘못판단된 Loss
	-> 인테그랄의 R1과 R2는 해당 영역에서의 적분을 의미!
	
---------------------------------------------------------------------
p. 28

강의 자료에 적은것처럼 식을 바꾸고 나면

	-> 남은건 R1값뿐이다! R1을 찾아야 한다
	-> 기댓값 R을 최소로 하는 R1을 찾는다!

---------------------------------------------------------------------
p. 29, 30

g(x) 가 양수와 음수를 넘나드는 함수라면

	-> 음수쪽 부분만 선택하면 된다

	-> R1을 기준으로 0보다 작으면 w1, 반대는 w2

	** 수업시간에 다시 확인하기**

---------------------------------------------------------------------
p. 31 ~ 33

C11, C22의 코스트는 당연히 0이고

prior가 같다고 했을때
	-> 어디가 최적의 boarder인가를 찾는 문제!!

	-> 두 Likelihood는 주어졌고, (P(x|w1), P(x|w2))
	-> 앞의 식에 그대로 적용해서 풀면...
	-> x가 4.73과 1.27이 나왔다.
	-> 따라서 Loss를 최소화 하는 boarder를 구할 수 있었다!!!
		-> 식을통해 구해보니 3개의 구역으로 나뉘었다!

---------------------------------------------------------------------
p. 34

Likelihood Ratio Test의 여러가지 버전

	-> 베이즈의 표준식과,
	-> MAP에서의 표준 
	-> Maximum Likelihood의 표준



---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
