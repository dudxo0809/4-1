p. 3

s : t 시점의 상태 (시점 t와 파라미터 세타를 가짐)

	-> 이전 시점의 S를 함수 F에 넣어 현재 시점의 상태를 계산
	-> t시점의 S를 풀어(unfolded)로 나타낸 그림 p.4

---------------------------------------------------------------------
p. 5

X 는 매 시점 t마다 외부에서 들어오는 input값임
	-> S(t) : t-1시점의 state와 t시점의 x와 파라미터 세타값을 F에 넣은 output

h(t) : hidden state에서의 state, t시점에서



---------------------------------------------------------------------
p. 6

unfolded 형태

	-> 이전상태의 hidden state가 현재 상태의 hidden state에 영향을 주므로
	-> Recurrent의 형태 : RNN!

	-> t-1 시점의 hidden state와 t 시점의 input X가 
		t시점의 hidden state를 결정!

---------------------------------------------------------------------
p. 8~

RNN에서 나오는 상태값 외에도 output값이 존재함

함수를 재정의 하면...
	-> t시점에서의 hidden state는
	-> t-1시점의 hidden state와 t시점의 input x 그리고
	-> weight(파라미터)를 밖으로 빼서 사용

	-> 

---------------------------------------------------------------------
p. 11

Input도 많고, output도 많은 케이스> ex. 자동번역 등...

Whh, Wxh, Why를 다 합쳐 W로 표현

L : 지도학습의 경우에 사용
	-> Loss 값: 누적됨!
	
전체 t시간이 지나면 누적된 Loss값을 통해 역전파를 진행할 수 있음!

	-> 현재 그림은 레이블이 있는 데이터를 통해 학습

---------------------------------------------------------------------
p. 13

Input은 많고, output은 하나인 케이스>ex. 감정분석(Sentimental Analysis)
				-> 글을 보고 긍정/부정 판단

input은 하나, output은 여러개인 케이스>ex. 이미지 캡셔닝등

---------------------------------------------------------------------
p. 14

두개가 섞인 Encoder-Decoder구조도 있음

---------------------------------------------------------------------
p. 15

character level 언어 모델

	-> h, e, l, l 을 input으로 넣어 학습

	-> Input Layer에서 t시점마다 char하나식 입력
	-> 각 시점에서 hidden layer에서 연산 진행
	-> 다시 각 시점마다 output layer에서 결과값이 도출됨

	타겟값은 ello임!
	-> 각 시점에서 타겟을 다음시점에서의 hidden으로 넣어줄 수 있음
		-> teacher forcing : 강제로 학습

---------------------------------------------------------------------
p. 19

t시점에서부터 1시점까지로 역전파를 진행하면서, weight를 업데이트함

1. 먼저 timestamp를 따라 Loss값을 모음
2. Loss값을 토대로 역전파를 진행

---------------------------------------------------------------------
p. 20

Gradient Vanishing Problem이 발생할 수 있음

	-> T가 길어지면 길어질수록, 이전의 Loss값은 영향을 잘 못미침
		-> weight값의 변화가 매우 적어지게 됨!

	-> 일정 구간마다 Loss값을 누적해서 update하는 방식을 방법
	-> Truncated-BTT 방식 (Backpropagation Through Time)


---------------------------------------------------------------------
p. 21

one-to-Many의 예제

이미지 캡셔닝

input으로 이미지를 넣고 주요 단어들을 출력함

---------------------------------------------------------------------
p. 35

RNN의 문제

너무 이전 시점의 데이터는 큰 영향을 못미친다.
이를 위해 LSTM을 고안함


---------------------------------------------------------------------
p. 36

Long-Short term Memory
LSTM

LSTM은 RNN의 것들을 포함하고, C를 새로 가짐
	-> C는 일부 영향을 미칠수도 아닐 수도 있다.

---------------------------------------------------------------------
p. 37

이전 RNN의 Cell은 하이퍼볼릭탄젠트연산 하나만 진행했었다.

그러나 LSTM의 Cell은 더 복잡함
	-> 시그모이드가 추가됨

---------------------------------------------------------------------
p. 38

LSTM의 셀에는 3개의 gate가 존재함

	-> Forget gate, Input gate, Output gate
	-> 모두 input은 똑같다 (x^t, h^t-1)

	-> 각 게이트와 z연산마다 식을 통해 값을 계산함


---------------------------------------------------------------------
p. 39









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
