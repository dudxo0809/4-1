p. 4, 5

엔트로피 :불확실성의 정도

정보의 양이 많으몀 확률은 작고
정보의 양이 적다면 확률은 크다

Shannon Entropy : 정보의 양과, 확률에 대한 기대치
	-> 각 이벤트의 값에 확률을 곱해 전부 더함

목표 : 모델이 확실한 output을 내줘야함
	-> 엔트로피가 작아야한다
Cross Entropy : 
	Q : 모델에 의해 나오는 확률 (CNN output)
	-> 이 함수를 Loss func으로 사용할 것

	
	



---------------------------------------------------------------------
p. 6

Output unit으로는 softmax를 주로 사용함

	K개의 클래스 분류문제
	




---------------------------------------------------------------------
p. 11

Parameter Norm Penalty

	-> 





---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
