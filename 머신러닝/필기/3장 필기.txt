p. 2

Random Variable

	-> 모든 이벤트, 랜덤한 상황들을 Function으로 만드는것
	-> ex.) 주사위 : y=1/6 의 함수, 가우시안, etc...

	-> 모든 경우를 x축에 매핑, y는 각 event에대한 확률


---------------------------------------------------------------------
p. 3

확률분포

ex.) N개의 동전을 동시에 던진다고 가정
	-> N개의 동전이 앞면일때의 모든 event에대한 확률
	-> 이 모든 데이터들을 함수로 표현

	Random variable에도 다양한 성질, 규칙이 있다


샘플?

	우리가 만든 모델을 이용해서 가짜 데이터들을 만드는것
	-> 랜덤프로세스라는 함수를 만들고 이 함수를 따르는 값들을 생성하는것


---------------------------------------------------------------------
p. 4, 5

패스







---------------------------------------------------------------------
p. 6

pdf는 연속적이고, pmf는 이산적이다.

	pmf는 pdf의 이산적인 버전





---------------------------------------------------------------------
p. 7

Expectation : 랜덤변수의 평균값

ex.) 점수 평균값 : 1/8 * (90 + 80 + 70 + 70 + 80 + 80 + 90 + 90)
		= 2/8 (70) + 3/8 (90) + 3/8 (80) 
		-> 여기서 분수는 각 데이터를 대변하는 확률로 나타낼 수 있다.
		-> 이렇게 바꾸는것이 expectation function임!


---------------------------------------------------------------------
p. 8

분산 역시 probability density function으로 표현할 수 있다.

	-> 분산 : 얼마나 평균에서 떨어져 있는가
	-> 평균으로부터의 거리가 같은 데이터들끼리의 확률을 묶음



---------------------------------------------------------------------
p. 9

Vector Random Variable

	-> random variable은 어떤 event를 숫자로 표현한것
	
	-> 여러 채널의 데이터들을 rnadom variable로 보고 vector로 표현
	-> random number라고해서 마구잡이 데이터가 아니라
		-> random process를 따르는 특정한 패턴의 띄는 데이터를 의미
		-> ex.) 가우시안 분포를 따르는 random variable
		-> mathematical한 모델을 따르는 변수가 random variable

	-> 다변량의 데이터를 벡터로 표현할 수 있다.

---------------------------------------------------------------------
p. 11

Marginal Probability Density Function

	-> 여러변수의 특정 사건이 있을때 하나의 사건(하나의 변수)에 대해 sum한것
	-> 




---------------------------------------------------------------------
p. 12

Covariance Matrix

	-> 다변량 데이터에서 각 채널에서의 데이터의 평균을 u로 하고
	-> 각 u를 묶어 하나의 벡터로 보자

	-> 각 u가 시계열 데이터일때 이 데이터 u는 가우시안 분포로 나타낼 수 있음
		-> 각 u는 random variable로 나타낼 수 있음

Random Matrix 

	-> 여러 데이터를 nxn으로 묶어 표현할 수 있음
	-> 영상 데이터에서 각 픽셀의 데이터는 random matrix로 표현가능

---------------------------------------------------------------------
p. 13

Covariance Matrix

	-> 함수의 앞에 Y-u에서 u는 평균값임
		-> 패턴은 일정하지만 전체적으로 값이 올라가 있거나 내려가 있는경우때문
			-> DC bias
	-> 행렬에서의 diagonal은 해당 신호(자기자신)의 power값이다.
	-> diagonal을 제외한 부분은 상관관계를 의미
		-> 1행4열의 값은 1번 데이터와 4번 데이터의 상관관계
		-> 비슷한 시계열 데이터를 dot product하면 양의 값이나옴
		-> 비슷한 상관관계(covariance)

	이렇게 Covariance Matrix를 계산하면
	여러 데이터(다변량 데이터)에서의 각 데이터들마다의 
	상관관계를 계산할 수 있다!!!


Covariance Matrix를 3개로 나눌 수 있다
	
	-> 가운데 행렬이 correlation
	-> correlation은 covariance를 normalize한것!
	-> 양쪽 행렬은 각각이 분산을 뜻함
		-> 분산의 제곱이 표준편차이고
		-> covariance를 표준편차로 나눈것이 correlation이다!!!

---------------------------------------------------------------------
p. 15, 16

가우시안 확률 분포

	-> 정규분포곡선 처럼 exp함수를 사용해 분포를 나타내는 함수

	-> 다변량 데이터의 벡터 x를 가우시안으로 나타낼 수 있다
	-> 시그마는 covariance matrix (diagonal : 각 채널의 power, 나머지 : 채널간의 correlation)
		-> 이 matrix는 symmetric matrix임 (대칭 행렬)


---------------------------------------------------------------------
p. 17

Central limit theorem

	-> 샘플의 크기가 커질 수록
		-> 분산은 작아진다!!!

---------------------------------------------------------------------
p. 18

주사위 1개는 uniform distribution이지만

	-> 주사위 갯수를 늘릴 수록
	-> 가우시안 분포를 따르게 된다.

---------------------------------------------------------------------
p. 19

2개 변수에서의 가우시안 예제

	x에서의 평균이 75, y에서의 평균이 110
	-> covariance matrix를 보면 두 변수간의 상관관계가 8400이다.
		-> correlation	

	3차원그래프에서의 y축은 빈도수임
	-> 높을수록 데이터가 많이있다.

20p에서의 가우시안 분포는 앞의 분포와 평균은 같다

	-> 하지만 correlation이 0이다.

21p에서의 분포를 보면 y축의 분산이 작아졌다.
	-> 여전히 correlation은 0이다.


---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
