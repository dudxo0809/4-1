p. 2

클러스터링에 관해 배우기에 앞서

지도학습과 비지도 학습에 대해 알아보자

	Supervised : 레이블이 있는 데이터로 학습
		-> 신경망, SVM
	
	Unsupervised : 레이블이 없는 데이터로 학습
		-> 클러스터링

	데이터 레이블링이 빡세서 비지도 학습에 많은 관심, 연구

---------------------------------------------------------------------
p. 3, 4

Clustering

	-> 클러스터링은 상황에 따라 달라진다.
	-> Skim : 기준에 따라 달라진다

	데이터 X, 각 데이터에는 여러 특성이 있다.
	-> N개의 데이터에 d개의 특성
	-> k개의 클러스터로 그룹지을 수 있음

	-> 컴퓨터가 클러스터링을 할때의 기준?
		-> 샘플들이 얼마나 비슷한지
	-> 샘플들간의 차이를 계산하는 객관적인 방법이 매우 중요하다!

---------------------------------------------------------------------
p. 5

Distance

	-> Minkowski distance : 두 샘플간의 거리를 계산
		-> 거리를 계산하고 절댓값을 취함
	-> order에 따라 식이 달라지는데, 가장 유명한건 유클리드임

Similarity

	-> 벡터의 내적에서의 코사인값을 이용

---------------------------------------------------------------------
p. 7

Group Distance

	-> 그룹끼리의 거리는 어떻게 계산하나?
		-> 평균!
	-> 그룹과 샘플은?
	-> 가장 거리가 먼 샘플과, 가장 거리가 가가운 샘플을 찾으면
		-> Outlie를 찾는데 좋다

	-> 그룹에서의 평균에 가장 가까운값을 그룹의 대푯값으로 한다


---------------------------------------------------------------------
p. 9

샘플의 각 데이터마다 다른데이터간의 거리를 구해보니
	-> y3가 가장 작았다
	-> y3가 대표가 됨

---------------------------------------------------------------------
p. 13

Hierarchical Clustering

작은그룹에서 큰 그룹으로 올라가는 Agglomerative와
큰 그룹에서 작은 그룹으로 내려가는 Divisive가 있다.

---------------------------------------------------------------------
p. 14

Agglomerative 방식

	-> 이 알고리즘은 샘플 X에서 각 데이터를 하나의 클러스터로 보고
	-> 하나씩 묶어주는 방식이다.

	-> 샘플이 7개면 7개의 클러스터로 시작
	-> 루프를 반복하면서 Dmin을 이용해서 계산한다
	-> 1과 2가 가장먼저 그룹핑이 되었다?
		-> 저 둘이 제일 가깝기 때문 (Dmin으로 계산)
	-> 한번의 그루핑이 되고나서 다시 Dmin을 사용해 계산
		-> 한 샘플과 다른 그룹의 distance중 가장 짧은값을 계산(Dmin)

	최종적으로는 하나의 클러스터가 되는데 이제는 사람의 판단이 필요함

---------------------------------------------------------------------
p. 18

만약 이 Agglomerative알고리즘에서 Dmax를 사용한다면???

	5번과 7번이 먼저 묶였다?
	-> 앞에서는 2번과 3번이 가깝다고해서 1234가 하나의 클러스터가 되었음
	
	-> Distance를 사용하는 기준만 바뀌어도 여러가지가 바뀔 수 있다.

---------------------------------------------------------------------
p. 20 

Single Linkage 는 Dmin을 사용하고 길쭉한 형태로 클러스터
Complete Linkage는 Dmax를 사용하고 동그란 형태로 클러스터
Average Linkage는 Dave를 사용하고 둘 사이의 어떤형태로 클러스터링됨


몇개의 클러스터로 클러스터링할지는 user가 결정하는것!
	-> 이를 자동적으로 결정하게하는것은 매우 어렵다

이 single/complete linkage방식은 Outlier에 민감하지만
	average방식은 덜하다

이 Agglomerative Hierarchical Clustering 방식은
	-> 시간복잡도가 N^3  (샘플갯수 : N)이고 너무 complex하다!

---------------------------------------------------------------------
p. 22

Partitional Clustering

	-> 좀더 효율적으로 클러스터링을 하기위한 방법
	-> 한 대푯값을 사용해서 클러스터링 (앞의 Hierarchical 방법보다 효율적)

---------------------------------------------------------------------
p. 23

K-means algorithm

	-> 가장 대중적인 클러스터링 알고리즘
	-> 시작에 앞서 몇개의 클러스터를 할지 정해야한다.

예시

	7개의 샘플을 3개의 클러스터한다고 가정
	-> 무작위 3개의 샘플을 기준으로 클러스터
	-> 클러스터의 센터를 다시 계산하고 다시 클러스터링함

K-Means 알고리즘은

	-> 초기의 센터에 민감하고, 빠르고, Outlier에 민감하다.


---------------------------------------------------------------------
p. 27

Hierarchical은 모든 샘플들의 distance를 전부 계산
	
Partitianal은 클러스터의 갯수만큼만 initial에따라 계산

---------------------------------------------------------------------
p. 28

Gaussian Mixture Model

1차원에서 3개의 클러스터, 2차원에서 6개의 클러스터로 구분할수 있을듯

	-> 전체 pdf에 여러개의 가우시안이 있을것이라고 가정한것
	-> 각 클러스터가 각각의 pdf를 따른다


---------------------------------------------------------------------
p. 29

강의 자료의 식에서 wi는 각 클러스터(state)를 의미

	-> 여기서 P(X)는 전체 pdf그 자체를 의미
	-> P(X|세타)에서 이 세타는 가우시안 혼합모델에서의 파라미터이므로
		-> 세타는 각 가우시안에 대한 평균과 분산이다.

	-> 강의자료는 M개의 클러스터가 mixture된 모델을 의미
	-> 강의자료의 equation이 수학적으로 왜 성립하나?
		-> Marginalization!!
		-> 
11분 00초!

---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
