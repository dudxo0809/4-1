p. 2

머신러닝을 100% 확신의 결정으로 보지 않고
확률의 문제로 보자

Uncertainty의 아래에있는이상
	-> 확률과 통계에 기반할 수 밖에 없다.



---------------------------------------------------------------------
p. 3, 4, 5

Statistics에 대한 여러 Terminology

Population : 분석을 위한 모든 데이터
Samples : 전체 데이터중 일부 데이터
	-> Population은 얻기 매우 힘들기 때문에 sample을 사용
Sample distribution : 데이터가 어떻게 분포되어 있는가
	-> population에 대한 distribution은 찾기 어렵다
	-> samples에 대한 distribution은 비교적 찾기 쉽다


통계 

<데이터 분포 분석을 위한 4개의 statistic order parameter>

Mean : 평균
	-> 1st order parameter

Variance : 분산
	-> 분산이 작을 수록 좋다 : 평균으로부터 데이터가 얼마나 퍼져있는가
	-> 2nd order parameter

Skewness : 데이터가 얼마나 치우쳐져 있는가
	-> 3rd order parameter
	-> negative : 오른쪽으로 치우침 / positive : 왼쪽으로 치우침
	
Kurtosis : 데이터의 분포가 얼마나 납작or뾰족한가
	-> 4th order parameter
	-> ex.) 하나의 색상에 몰렸는지, 여러 색상의 동물 수가 동일한지

-> 각 차수마다 x의 n승 의 평균이다.

---------------------------------------------------------------------
p. 6, 7

Covariance : 두 변수간에 얼마나 상관관계가 있는가?
	-> 핑크 : x가 늘수록 y도 는다 (covariance가 높다)
	-> 주황 : y값에 관계없이 x값이 일정하다 (covariance가 낮다)
	-> 만약 covariance가 0이면 두 변수간에 상관관계가 아예없다는 뜻이다!

	-> 각 x값과 y값을 곱해 이를 모두 더함
		-> 벡터의 내적과 같다!!!
		-> 하지만 상관관계를 비교하기에는 아직 부족

Correlation : 각 분포에 대해 표준편차로 나눠 covariance를 normalize한것

	-> 이렇게하면 순수하게 x와 y의 상관관계만 구할 수 있다.
	-> 표준편차는 데이터의 크기를 의미
		-> 표준편차에 루트를 씌운게 분산
		-> 분산or표준편차는 데이터의 scale을 의미

---------------------------------------------------------------------
p. 8

3개의 서로다른 변수 x1~3

	-> x1과 x2의 correlation은 0.46정도
	-> x2와 x3의 correlation은 -0.99이다! 

	-> 따라서 x3가 x1보다 x2와의 상관관계가 더 크다
	-> ex.) 결석 횟수가 도서관 공부시간보다 성적에 큰 영향을 준다는 분석!


---------------------------------------------------------------------
p. 9

Bias : 실제값과 데이터들이 얼마나 떨어져 있는가

ex.) 휴대폰의 크기와 실 사용자의 선호도 관계





---------------------------------------------------------------------
p. 10

에러 : 노이즈+bias+variance
	-> 노이즈 : 어쩔수 없이 데이터에 생기는 값
	-> bias와 variance는 모델에 따라 줄일 수 있다


Bias와 Variance의 관계??

	-> x와 y의 관계를 수식화하여 model을 찾는 regression문제를 가정
	-> 단순히 일차함수를 이용해서 regression모델을 만들면?
		-> 에러가 있지만 정답에 근접함
	-> but, 이차함수 모델에 비하면 under fit이다
	-> 그런데 2차에서 아주 조금 존재하는 오차를 제거하기 위해
		-> 5차 6차의 모델을 만들면?
		-> 현재의 data에만 정확히 맞추는 overfit된 모델이다!
		-> 새로운 값을 예측할때 정답과 오차가 크다!
		
		-> underfit과 overfit중 어느게 더 오차가 클지는 모름!

	underfit은 bias가 크고(실제값을 잘 못맞춰서)
		-> 직선위에 있으므로 variance는 작다
	overfit은 variance가 크다 (bias가 거의 0)

---------------------------------------------------------------------
p. 11~15

Probability에는 항상 일정한 확률의 neumerical 과
항상 확률이 변하는 statistic이 있다

Event에는 동시에 일어날 수 있는 사건과
	동시에 일어날 수 없는 exclusive event가 있다.


Sample : 모든 경우중 일부경우

Probability space : 일어날 수 있는 모든 경우

---------------------------------------------------------------------
p. 16

조건부로 존재하는 Conditional Probability가 있다.

사건A와 사건B가 동시에 일어날 확률인 Joint Probability가 있다.
	-> simultaneously가 항상 시간적으로를 의미하는것은 아님!
		-> ex.) 어제도 치킨을먹고 오늘도 치킨을 먹을 확률
	-> 두 사건은 독립적임을 가정한다 (각 사건이 서로 영향을 주면 안됨)


---------------------------------------------------------------------
p. 18

Chain Rule

	-> 여러 사건에 대한 Joint probability를 풀어서 쓴것
	-> ex.) 오늘,어제,그제의 비올 확률
	-> P(w0,w-1) = P(w0|w-1) * P(w-1) // 독립적이라고 가정
	-> // (어제의 확률에 기반한 오늘의 확률) * (어제의 확률)

	P(w0,w-1,w-1) = P(w0,w-1|w-2) * P(w-2)
		= P(w0|w-1,w-2) * P(w-1|w-2) * P(w-2)

---------------------------------------------------------------------
p. 20

Marginal Probability

	-> 이 테이블은 각 요일에 어떤 운동을 하게될 확률에 대한 표다.
	-> 테이블의 각 확률은 joint 확률이다.
	-> 월요일에 운동할 확률은 16/32이고
		월화수목에 축구를 할 확률은 8/32이다.

	-> P(A,B)가 있을때 P(A)만 구하는 것을 Marginalize한다고 한다.
	-> P(A,B0) + P(A,B1) + ... + P(A,Bn) 의 값 : B에대해 시그마를 취함

---------------------------------------------------------------------
p. 21

Bayes' Theorem

	-> P(A|B)는 아는데, P(B|A)를 알고싶을 때 사용
	-> 사전지식(Prior)이 있을때 


---------------------------------------------------------------------
p. 

베이즈 다시보기







---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
