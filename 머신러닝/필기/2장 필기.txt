p. 2

머신러닝을 100% 확신의 결정으로 보지 않고
확률의 문제로 보자

Uncertainty의 아래에있는이상
	-> 확률과 통계에 기반할 수 밖에 없다.



---------------------------------------------------------------------
p. 3, 4, 5

Statistics에 대한 여러 Terminology

Population : 분석을 위한 모든 데이터
Samples : 전체 데이터중 일부 데이터
	-> Population은 얻기 매우 힘들기 때문에 sample을 사용
Sample distribution : 데이터가 어떻게 분포되어 있는가
	-> population에 대한 distribution은 찾기 어렵다
	-> samples에 대한 distribution은 비교적 찾기 쉽다


통계 

<데이터 분포 분석을 위한 4개의 statistic order parameter>

Mean : 평균
	-> 1st order parameter

Variance : 분산
	-> 분산이 작을 수록 좋다
	-> 2nd order parameter

Skewness : 데이터가 얼마나 치우쳐져 있는가
	-> 3rd order parameter
	-> negative : 오른쪽으로 치우침 / positive : 왼쪽으로 치우침
	
Kurtosis : 데이터의 분포가 얼마나 납작or뾰족한가
	-> 4th order parameter
	-> ex.) 하나의 색상에 몰렸는지, 여러 색상의 동물 수가 동일한지

-> 각 차수마다 x의 n승 의 평균이다.

---------------------------------------------------------------------
p. 6, 7

Covariance : 두 변수간에 얼마나 상관관계가 있는가?
	-> 핑크 : x가 늘수록 y도 는다 (covariance가 높다)
	-> 주황 : y값에 관계없이 x값이 일정하다 (covariance가 낮다)
	-> 만약 covariance가 0이면 두 변수간에 상관관계가 아예없다는 뜻이다!

	-> 각 x값과 y값을 곱해 이를 모두 더함
		-> 벡터의 내적과 같다!!!
		-> 하지만 상관관계를 비교하기에는 아직 부족

Correlation : 각 분포에 대해 표준편차로 나눠 covariance를 normalize한것

	-> 이렇게하면 순수하게 x와 y의 상관관계만 구할 수 있다.


---------------------------------------------------------------------
p. 8

3개의 서로다른 변수 x1~3

	-> x1과 x2의 correlation은 0.46정도
	-> x2와 x3의 correlation은 -0.99이다! 

	-> 따라서 x3가 x1보다 x2와의 상관관계가 더 크다
	-> ex.) 결석 횟수가 도서관 공부시간보다 성적에 큰 영향을 준다는 분석!


---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
