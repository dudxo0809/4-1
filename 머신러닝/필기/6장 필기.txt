p. 5

Linear Regression

	-> 먼저 Cost function을 만들어야한다.
	-> 데이터의 패턴을 보고 어떤 모델을 쓸지 정함(선형회귀 등...)
	-> 파라미터 세타0,1을 찾기 위한 cost func을 정해야 한다

	-> 샘플 12개가 있다면 이 샘플전체에 대해 샘플의 y값과 모델의 y값의 차이
	-> 이 차이 값들을 모두 더한게 cost 값이고
	-> 이 값을 최소화 하는 파라미터값 세타0,1을 찾아야한다.

		-> 제곱? : 거리는 음수이면 안됨

---------------------------------------------------------------------
p. 12

알파값은 learning rate

	-> 이 값이 너무 작으면 gradient descent가 느리다.
	-> 너무 크면 overshoot되어 minimum으로 가기 힘들다

	-> gradient descent가 진행될수록, 알파값은 알아서 줄어들게 된다!


---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
