p. 









---------------------------------------------------------------------
p. 6~

벡터 weight와 input X의 Dot Product값으로 classifying한다.
	-> 벡터 하나를 Transpose
	-> F(x)
	이 F(x)값과 weight의 cosine 값에 따라 유사도를 판단!

	-> weight의 free와 money는 특성이고,
		벡터 X의 값과 Linear Combination된다.
	-> BIAS가 0이면 weight(hyperplane)가 원점을 지남
		BIAS는 원점에서 hyperplane을 얼만큼 이동할것인가

	

---------------------------------------------------------------------
p. 8

Hyperplane을 기준으로 classification

	-> 만약 오 분류된 data가 들어오면 이에 맞게 조절
	-> X에 맞게 weight를 update하는것이 Learning!

---------------------------------------------------------------------
p. 9

Binary Perceptron

	1. Weight = 0 으로 시작
	2. training sample이 들어오면 현재 w를 기준으로 classification
		-> y값이 나옴 : -1 or +1
	3. 만약 분류가 잘 되었다면 no change (Y* : Label)
		-> 잘못 분류가 되면 : w를 수정함
	   새로운 w와 y벡터를 dot했을때 음수값이 나오게 수정해야함!!

		w = w + (y*) * f
		기존의 w벡터에서 f만큼을 subtract!!
		-> 새로운 w벡터의 값은 핑크색이됨!
		


---------------------------------------------------------------------
p. 10 ~

클래스가 여러개일때의 Perceptron 학습

	-> 여러개의 weight중, w * F(x) 를 최대로 하는 y로 판단한다.

잘못 판단시)

	-> 잘못 인식된 weight는 subtract
	-> 원래 Label에 따라 score가 높아야하는데 낮았던 클래스의w에대해
		add함



---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
