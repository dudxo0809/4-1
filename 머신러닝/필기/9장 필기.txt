p. 5

퍼셉트론이 아니라 뉴런을 사용하자

	-> 함수 F가 다양한 함수를 쓸수있게 함
	-> 이전의 input 값들을 합쳐서 F에 넣음
	-> F를 통과해 나온 값이 뉴런의 output 값



---------------------------------------------------------------------
p. 6

이렇게 구축된 네트워크가 

Feedforward Neural Network이다. 

	-> input 값들이 뉴런을 통과해 새로운 output을 만들고
	-> 이 값이 다시 뉴런을 통과해 새로운 output을 만든다.

	지도학습 : 샘플데이터를 준다.
		-> 원하는 input, output값이 존재


---------------------------------------------------------------------
p. 7

함수 F에는 여러 선택지가 있다.

	-> Linear func : 선형 회귀의 형태
	-> Symmetric Hard Limit Trans func : 퍼셉트론과 같은 형태
	-> Satlin Transfer func : 0과 1에 수렴
	-> Tan Sigmoid Transfer func : 연속적이기 때문에 미분가능
	-> Log Sigmoid : 양방향이 0과 1에 수렴
	-> Radial Basis func : 

---------------------------------------------------------------------
p. 8

Backpropagation : 역전파

양방향의 에러가 최저가 되도록 하는 F를 찾아야한다
	-> 각 학습데이터에는 Label이 있어 원하는 값을 찾아가게 한다.
	-> 입력된 데이터는 뉴런을 거쳐 다음 계층을 따라 전해진다
		-> Percolating
	-> 최종적으로 나온 output과 우리가 기대하는 값의 오차를 구한다.
	-> 이 값을 역전파 하면서 weight를 조정한다.


---------------------------------------------------------------------
p. 9

Learning Rule : 각 뉴런을 중심으로한 학습 과정

	-> 초기에는 weight가 랜덤값으로 되어있다.
	-> F를 거쳐 output이 나옴
	-> 원하는 값(desire output)과 실제 output의 차이를 계산
	-> 이 오차값을 사용해 weight 값을 수정함

	-> 여기서 n은 learning rate이다.
		-> 얼마나 weight를 변화시킬지에 대한 상수값

---------------------------------------------------------------------
p. 10

에러 측정 : F를 통과한 X값(output)과 Y값(desire output)의 차이
	-> feature의 갯수 N : 모든 y에 대한 오차를 더해 에러값을 구함

각 feature의 weight에 대해 편미분을 함
	-> 이전의 에러를 weight에 대해 미분한 값(기울기)
	-> 이 값을 구해 weight를 수정함


---------------------------------------------------------------------
p. 11

Sigmoid 함수

	-> 왼쪽 방향은 0, 오른쪽 방향은 1에 수렴함
	-> 타우 값, net값에 따라 곡선이 꺾이는 정도가 달라짐
	-> 보통 타우를 0or1로 두고 사용

---------------------------------------------------------------------
p. 13

e의 지수를 단순화 시켜 x로 두었다.

	-> 이 sigmoid func을 미분하면
	-> S'(x) = S(x)(1-S(x)) 의 꼴이 된다.

	계산이 매우 효율적!

---------------------------------------------------------------------
p. 14, 15

p : p번째 샘플, k : output의 길이, k번째 output

Training Set : X와 D가 존재하는 데이터 셋
	d : desire output


P개의 패턴

MSE : Mean Square Error

	-> P개의 패턴 전체와, K개의 output에 대해 오차를 제곱후 더함

---------------------------------------------------------------------
p. 16~

3개의 input을 넣어 최종적으로 2개의 output

Hidden Layer : input계층과 output계층 사이의 계층
	Wj,i (1,0) : 1번째 hidden Layer에서의 
		i번째뉴런->j번째 뉴런 방향으로가는 weight

Error를 줄이는 방향으로 weight가 조정됨
	-> 미분값: 기울기를따라 최저점으로 내려가는 방식

	Weight의 변화량은 : 에러를 weight로 편미분한 값과 비례한다.
	-> 모든 weight에 대해 이를 수행해야 에러를 최소화 할 수 있다.
	-> 하지만 hidden layer에서의 편미분은 쉽지 않다.

---------------------------------------------------------------------
p. 19

2번째 hidden layer의 j->k로의 weight

	-> 특정 k를 고정하고, 미분
	-> 




---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
p. 









---------------------------------------------------------------------
